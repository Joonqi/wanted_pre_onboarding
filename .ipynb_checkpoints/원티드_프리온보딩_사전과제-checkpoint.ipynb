{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7da1364",
   "metadata": {},
   "source": [
    "# Pre On Boarding assingment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6303bd0",
   "metadata": {},
   "source": [
    "### 1) Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb75543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        self.word_dict = {'oov' : 0}\n",
    "        self.fit_checker = False\n",
    "    \n",
    "    def preprocessing(self, sequences):\n",
    "        result = []\n",
    "        '''\n",
    "        문제 1-1.\n",
    "        '''\n",
    "        for sentence in sequences:\n",
    "            string = re.sub(r'[^a-zA-Z0-9가-힣 ]', '', sentence.lower())\n",
    "            result.append(string.split())\n",
    "        return result\n",
    "    \n",
    "    def fit(self, sequences):\n",
    "        self.fit_checker = False\n",
    "        '''\n",
    "        문제 1-2.\n",
    "        '''\n",
    "        tokens = self.preprocessing(sequences)\n",
    "        idx = 1\n",
    "        for token in tokens:\n",
    "            for word in token:\n",
    "                if word not in self.word_dict:\n",
    "                    self.word_dict[word] = idx\n",
    "                    idx += 1\n",
    "        self.fit_checker = True\n",
    "    \n",
    "    def transform(self, sequences):\n",
    "        result = []\n",
    "        tokens = self.preprocessing(sequences)\n",
    "        if self.fit_checker:\n",
    "            '''\n",
    "            문제 1-3.\n",
    "            '''\n",
    "            for token in tokens:\n",
    "                indexes = []\n",
    "                for word in token:\n",
    "                    if word in self.word_dict:\n",
    "                        indexes.append(self.word_dict[word])\n",
    "                    else:\n",
    "                        indexes.append(self.word_dict['oov'])\n",
    "                result.append(indexes)\n",
    "            return result\n",
    "        else:\n",
    "            raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
    "    \n",
    "    def fit_transform(self, sequences):\n",
    "        self.fit(sequences)\n",
    "        result = self.transform(sequences)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458234f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'go', 'to', 'school'], ['i', 'like', 'pizza'], ['i', 'dont', 'like', 'it']]\n"
     ]
    }
   ],
   "source": [
    "test = Tokenizer()\n",
    "test_input = ['I go to school.', 'I LIKE pizza!', \"I don't like it.\"]\n",
    "\n",
    "print(test.preprocessing(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1300e9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oov': 0, 'i': 1, 'go': 2, 'to': 3, 'school': 4, 'like': 5, 'pizza': 6}\n"
     ]
    }
   ],
   "source": [
    "test = Tokenizer()\n",
    "test_input = ['I go to school.', 'I LIKE pizza!']\n",
    "\n",
    "test.fit(test_input)\n",
    "print(test.word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f90f9187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4], [1, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "test = Tokenizer()\n",
    "test_input = ['I go to school.', 'I LIKE pizza!']\n",
    "\n",
    "test.fit(test_input)\n",
    "print(test.transform(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac7f28fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4], [1, 5, 6]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Tokenizer()\n",
    "test_input = ['I go to school.', 'I LIKE pizza!']\n",
    "\n",
    "test.fit_transform(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4803292f",
   "metadata": {},
   "source": [
    "### 2) TfIdf Vecotrizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "972338e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class TfidfVectorizer:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.fit_checker = False\n",
    "    \n",
    "    def fit(self, sequences):\n",
    "        tokenized = self.tokenizer.fit_transform(sequences)\n",
    "        '''\n",
    "        문제 2-1.\n",
    "        '''\n",
    "        # 문장의 갯수 N\n",
    "        N = len(tokenized)                                \n",
    "        # 말뭉치 전체에 포함된 단어 리스트 (oov 제외)\n",
    "        words = list(self.tokenizer.word_dict.values())[1:]   \n",
    "        \n",
    "        self.idf_vector = [0.0] * len(words)\n",
    "        \n",
    "        for i in range(len(words)):\n",
    "            df = 0\n",
    "            for sentence in tokenized:\n",
    "                df += words[i] in sentence\n",
    "            self.idf_vector[i] = math.log(N/(1+df))\n",
    "        self.fit_checker = True\n",
    "    \n",
    "    def transform(self, sequences):\n",
    "        if self.fit_checker:\n",
    "            tokenized = self.tokenizer.transform(sequences)\n",
    "            '''\n",
    "            문제 2-2.\n",
    "            '''\n",
    "            N = len(tokenized)\n",
    "            words = list(self.tokenizer.word_dict.values())[1:]\n",
    "            # (N x t) 형태의 중첩 리스트 정의\n",
    "            self.tfidf_matrix = [[0.0]*len(words) for i in range(N)]  \n",
    "            \n",
    "            for i in range(N):\n",
    "                sent_len = len(tokenized[i])\n",
    "                for j in range(len(words)):\n",
    "                    tf = (tokenized[i].count(words[j])) / sent_len\n",
    "                    tfidf = tf * self.idf_vector[j]\n",
    "                    self.tfidf_matrix[i][j] = tfidf\n",
    "            \n",
    "            return self.tfidf_matrix\n",
    "        else:\n",
    "            raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
    "        \n",
    "    def fit_transform(self, sequences):\n",
    "        self.fit(sequences)\n",
    "        return self.transform(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83cf1433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.40546510810816444, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = ['I go to school.', 'I LIKE pizza!']\n",
    "tfidf_test = TfidfVectorizer(test)\n",
    "\n",
    "tfidf_test.fit(test_input)\n",
    "tfidf_test.idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d1ac043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.10136627702704111, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [-0.1351550360360548, 0.0, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_test.transform(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5877d15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.28768207245178085,\n",
       " 0.28768207245178085,\n",
       " 0.6931471805599453,\n",
       " 0.28768207245178085,\n",
       " 0.6931471805599453,\n",
       " 0.6931471805599453,\n",
       " 0.6931471805599453,\n",
       " 0.6931471805599453,\n",
       " 0.6931471805599453]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = ['먹고 싶은 사과',\n",
    "              '먹고 싶은 바나나',\n",
    "              '길고 노란 바나나 바나나',\n",
    "              '저는 과일이 좋아요']\n",
    "\n",
    "test2 = Tokenizer()\n",
    "tfidf_test2 = TfidfVectorizer(test2)\n",
    "\n",
    "test2_output = tfidf_test2.fit_transform(test_input)\n",
    "tfidf_test2.idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27143d88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>먹고</th>\n",
       "      <th>싶은</th>\n",
       "      <th>사과</th>\n",
       "      <th>바나나</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>저는</th>\n",
       "      <th>과일이</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.095894</td>\n",
       "      <td>0.095894</td>\n",
       "      <td>0.231049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.095894</td>\n",
       "      <td>0.095894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143841</td>\n",
       "      <td>0.173287</td>\n",
       "      <td>0.173287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231049</td>\n",
       "      <td>0.231049</td>\n",
       "      <td>0.231049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         먹고        싶은        사과       바나나        길고        노란        저는  \\\n",
       "0  0.095894  0.095894  0.231049  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.095894  0.095894  0.000000  0.095894  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.143841  0.173287  0.173287  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.231049   \n",
       "\n",
       "        과일이       좋아요  \n",
       "0  0.000000  0.000000  \n",
       "1  0.000000  0.000000  \n",
       "2  0.000000  0.000000  \n",
       "3  0.231049  0.231049  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(test2_output, columns=list(test2.word_dict.keys())[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e809042a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "rs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
