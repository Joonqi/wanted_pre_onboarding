{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7da1364",
   "metadata": {},
   "source": [
    "# Pre On Boarding assingment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6303bd0",
   "metadata": {},
   "source": [
    "### 1) Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb75543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        self.word_dict = {'oov' : 0}\n",
    "        self.fit_checker = False\n",
    "    \n",
    "    def preprocessing(self, sequences):\n",
    "        result = []\n",
    "        for sentence in sequences:\n",
    "            string = re.sub(r'[^a-zA-Z0-9가-힣 ]', '', sentence.lower())\n",
    "            result.append(string.split())\n",
    "        return result\n",
    "    \n",
    "    def fit(self, sequences):\n",
    "        self.fit_checker = False\n",
    "        tokens = self.preprocessing(sequences)\n",
    "        idx = 1\n",
    "        for token in tokens:\n",
    "            for word in token:\n",
    "                if word not in self.word_dict:\n",
    "                    self.word_dict[word] = idx\n",
    "                    idx += 1\n",
    "        self.fit_checker = True\n",
    "    \n",
    "    def transform(self, sequences):\n",
    "        result = []\n",
    "        tokens = self.preprocessing(sequences)\n",
    "        if self.fit_checker:\n",
    "            for token in tokens:\n",
    "                indexes = []\n",
    "                for word in token:\n",
    "                    if word in self.word_dict:\n",
    "                        indexes.append(self.word_dict[word])\n",
    "                    else:\n",
    "                        indexes.append(self.word_dict['oov'])\n",
    "                result.append(indexes)\n",
    "            return result\n",
    "        else:\n",
    "            raise Exception('Tokenizer instance is not fitted yet')\n",
    "    \n",
    "    def fit_transform(self, sequences):\n",
    "        self.fit(sequences)\n",
    "        result = self.transform(sequences)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458234f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'go', 'to', 'school'], ['i', 'like', 'pizza'], ['i', 'dont', 'like', 'it']]\n"
     ]
    }
   ],
   "source": [
    "test = Tokenizer()\n",
    "test_input = ['I go to school.', 'I LIKE pizza!', \"I don't like it.\"]\n",
    "\n",
    "print(test.preprocessing(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1300e9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oov': 0, 'i': 1, 'go': 2, 'to': 3, 'school': 4, 'like': 5, 'pizza': 6}\n"
     ]
    }
   ],
   "source": [
    "test = Tokenizer()\n",
    "test_input = ['I go to school.', 'I LIKE pizza!']\n",
    "\n",
    "test.fit(test_input)\n",
    "print(test.word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f90f9187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4], [1, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "test = Tokenizer()\n",
    "test_input = ['I go to school.', 'I LIKE pizza!']\n",
    "\n",
    "test.fit(test_input)\n",
    "print(test.transform(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac7f28fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4], [1, 5, 6]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Tokenizer()\n",
    "test_input = ['I go to school.', 'I LIKE pizza!']\n",
    "\n",
    "test.fit_transform(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7f0746e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6, 7, 8],\n",
       " [9, 10, 11, 12, 13, 14, 15, 9, 16, 17, 18, 19],\n",
       " [9,\n",
       "  8,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  21,\n",
       "  9,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  13,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36],\n",
       " [1, 2, 37, 34, 38],\n",
       " [39, 40, 41, 7, 42, 43, 44, 45, 46, 23, 34, 47, 7],\n",
       " [39, 2, 48, 49, 21, 50, 51, 52, 8, 53, 26, 9, 54, 55, 56],\n",
       " [57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  1,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  21,\n",
       "  50,\n",
       "  67,\n",
       "  26,\n",
       "  64,\n",
       "  68,\n",
       "  7,\n",
       "  8,\n",
       "  34,\n",
       "  69,\n",
       "  70,\n",
       "  39,\n",
       "  63,\n",
       "  71,\n",
       "  21,\n",
       "  1,\n",
       "  72],\n",
       " [1,\n",
       "  73,\n",
       "  74,\n",
       "  70,\n",
       "  63,\n",
       "  75,\n",
       "  34,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  21,\n",
       "  80,\n",
       "  81,\n",
       "  34,\n",
       "  50,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  63,\n",
       "  86,\n",
       "  26,\n",
       "  87,\n",
       "  88],\n",
       " [1, 89, 74, 70, 63, 90, 34, 74, 50, 91, 92, 17, 64, 8, 93, 2, 94, 95, 96],\n",
       " [1, 97, 74, 98, 15, 99, 100, 63, 101],\n",
       " [1, 102, 103, 21, 104, 17, 64, 105, 106, 7, 107]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_python = [\"Python is an interpreted high-level general-purpose programming language.\",\n",
    "               \"Its design philosophy emphasizes code readability with its use of significant indentation.\",\n",
    "               \"Its language constructs as well as its object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\",\n",
    "               \"Python is dynamically-typed and garbage-collected.\",\n",
    "               \"It supports multiple programming paradigms, including structured (particularly, procedural), object-oriented and functional programming.\",\n",
    "               \"It is often described as a 'batteries included' language due to its comprehensive standard library.\",\n",
    "               \"Guido van Rossum began working on Python in the late 1980s, as a successor to the ABC programming language, and first released it in 1991 as Python 0.9.0.\",\n",
    "               \"Python 2.0 was released in 2000 and introduced new features, such as list comprehensions and a cycle-detecting garbage collection system (in addition to reference counting).\",\n",
    "               \"Python 3.0 was released in 2008 and was a major revision of the language that is not completely backward-compatible.\",\n",
    "               \"Python 2 was discontinued with version 2.7.18 in 2020.\",\n",
    "               \"Python consistently ranks as one of the most popular programming languages.\"]\n",
    "test1 = Tokenizer()\n",
    "test1.fit_transform(wiki_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4803292f",
   "metadata": {},
   "source": [
    "### 2) TfIdf Vecotrizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "972338e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class TfidfVectorizer:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.fit_checker = False\n",
    "    \n",
    "    def fit(self, sequences):\n",
    "        tokenized = self.tokenizer.fit_transform(sequences)\n",
    "        N = len(tokenized)                      # 문장의 갯수 N\n",
    "        words = list(self.tokenizer.word_dict.values())    # 말뭉치 전체에 포함된 단어 리스트\n",
    "        \n",
    "        self.idf_vector = [0.0]*len(words)\n",
    "        \n",
    "        for i in range(len(words)):\n",
    "            df = 0\n",
    "            for sentence in tokenized:\n",
    "                df += words[i] in sentence\n",
    "            self.idf_vector[i] = math.log(N/(1+df))\n",
    "        self.fit_checker = True\n",
    "    \n",
    "    def transform(self, sequences):\n",
    "        if self.fit_checker:\n",
    "            tokenized = self.tokenizer.transform(sequences)\n",
    "            N = len(tokenized)     # 문장의 갯수 N\n",
    "            words = list(self.tokenizer.word_dict.values())    # 말뭉치 전체에 포함된 단어 리스트\n",
    "            \n",
    "            # (N x t) 형태의 리스트 정의\n",
    "            self.tfidf_matrix = [[0.0]*len(words) for i in range(N)]\n",
    "            \n",
    "            for i in range(N):\n",
    "                sent_len = len(tokenized[i])\n",
    "                for j in range(len(words)):\n",
    "                    tf = (tokenized[i].count(words[j]))/sent_len\n",
    "                    tfidf = tf * self.idf_vector[j]\n",
    "                    self.tfidf_matrix[i][j] = tfidf\n",
    "            \n",
    "            return self.tfidf_matrix\n",
    "        else:\n",
    "            raise Exception('TfidfVectorizer instance is not fitted yet')\n",
    "        \n",
    "    def fit_transform(self, sequences):\n",
    "        self.fit(sequences)\n",
    "        return self.transform(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83cf1433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6931471805599453, -0.40546510810816444, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = ['I go to school.', 'I LIKE pizza!']\n",
    "tfidf_test = TfidfVectorizer(test)\n",
    "\n",
    "tfidf_test.fit(test_input)\n",
    "# 모든 문장에서 등장하는 단어의 경우 idf score = ln(N/1+N) 으로 음수로 계산된다.\n",
    "# 문장을 충분히 많이 입력하면 해결되는 문제라 생각\n",
    "tfidf_test.idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d1ac043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, -0.10136627702704111, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, -0.1351550360360548, 0.0, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_test.transform(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5877d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oov</th>\n",
       "      <th>먹고</th>\n",
       "      <th>싶은</th>\n",
       "      <th>사과</th>\n",
       "      <th>바나나</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>저는</th>\n",
       "      <th>과일이</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095894</td>\n",
       "      <td>0.095894</td>\n",
       "      <td>0.231049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095894</td>\n",
       "      <td>0.095894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143841</td>\n",
       "      <td>0.173287</td>\n",
       "      <td>0.173287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231049</td>\n",
       "      <td>0.231049</td>\n",
       "      <td>0.231049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   oov        먹고        싶은        사과       바나나        길고        노란        저는  \\\n",
       "0  0.0  0.095894  0.095894  0.231049  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.0  0.095894  0.095894  0.000000  0.095894  0.000000  0.000000  0.000000   \n",
       "2  0.0  0.000000  0.000000  0.000000  0.143841  0.173287  0.173287  0.000000   \n",
       "3  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.231049   \n",
       "\n",
       "        과일이       좋아요  \n",
       "0  0.000000  0.000000  \n",
       "1  0.000000  0.000000  \n",
       "2  0.000000  0.000000  \n",
       "3  0.231049  0.231049  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = ['먹고 싶은 사과',\n",
    "              '먹고 싶은 바나나',\n",
    "              '길고 노란 바나나 바나나',\n",
    "              '저는 과일이 좋아요']\n",
    "\n",
    "test2 = Tokenizer()\n",
    "tfidf_test2 = TfidfVectorizer(test2)\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(tfidf_test2.fit_transform(test_input), columns=list(test2.word_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "582a76b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oov</th>\n",
       "      <th>python</th>\n",
       "      <th>is</th>\n",
       "      <th>an</th>\n",
       "      <th>interpreted</th>\n",
       "      <th>highlevel</th>\n",
       "      <th>generalpurpose</th>\n",
       "      <th>programming</th>\n",
       "      <th>language</th>\n",
       "      <th>its</th>\n",
       "      <th>...</th>\n",
       "      <th>discontinued</th>\n",
       "      <th>version</th>\n",
       "      <th>2718</th>\n",
       "      <th>2020</th>\n",
       "      <th>consistently</th>\n",
       "      <th>ranks</th>\n",
       "      <th>one</th>\n",
       "      <th>most</th>\n",
       "      <th>popular</th>\n",
       "      <th>languages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039807</td>\n",
       "      <td>0.098557</td>\n",
       "      <td>0.213094</td>\n",
       "      <td>0.213094</td>\n",
       "      <td>0.213094</td>\n",
       "      <td>0.213094</td>\n",
       "      <td>0.098557</td>\n",
       "      <td>0.075767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.168600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027552</td>\n",
       "      <td>0.091964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063691</td>\n",
       "      <td>0.157691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040409</td>\n",
       "      <td>0.067440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028159</td>\n",
       "      <td>0.021648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016761</td>\n",
       "      <td>0.041498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189416</td>\n",
       "      <td>0.189416</td>\n",
       "      <td>0.189416</td>\n",
       "      <td>0.189416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154977</td>\n",
       "      <td>0.154977</td>\n",
       "      <td>0.154977</td>\n",
       "      <td>0.154977</td>\n",
       "      <td>0.154977</td>\n",
       "      <td>0.154977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    oov    python        is        an  interpreted  highlevel  generalpurpose  \\\n",
       "0   0.0  0.039807  0.098557  0.213094     0.213094   0.213094        0.213094   \n",
       "1   0.0  0.000000  0.000000  0.000000     0.000000   0.000000        0.000000   \n",
       "2   0.0  0.000000  0.000000  0.000000     0.000000   0.000000        0.000000   \n",
       "3   0.0  0.063691  0.157691  0.000000     0.000000   0.000000        0.000000   \n",
       "4   0.0  0.000000  0.000000  0.000000     0.000000   0.000000        0.000000   \n",
       "5   0.0  0.000000  0.052564  0.000000     0.000000   0.000000        0.000000   \n",
       "6   0.0  0.022747  0.000000  0.000000     0.000000   0.000000        0.000000   \n",
       "7   0.0  0.012738  0.000000  0.000000     0.000000   0.000000        0.000000   \n",
       "8   0.0  0.016761  0.041498  0.000000     0.000000   0.000000        0.000000   \n",
       "9   0.0  0.035384  0.000000  0.000000     0.000000   0.000000        0.000000   \n",
       "10  0.0  0.028950  0.000000  0.000000     0.000000   0.000000        0.000000   \n",
       "\n",
       "    programming  language       its  ...  discontinued   version      2718  \\\n",
       "0      0.098557  0.075767  0.000000  ...      0.000000  0.000000  0.000000   \n",
       "1      0.000000  0.000000  0.168600  ...      0.000000  0.000000  0.000000   \n",
       "2      0.000000  0.027552  0.091964  ...      0.000000  0.000000  0.000000   \n",
       "3      0.000000  0.000000  0.000000  ...      0.000000  0.000000  0.000000   \n",
       "4      0.121301  0.000000  0.000000  ...      0.000000  0.000000  0.000000   \n",
       "5      0.000000  0.040409  0.067440  ...      0.000000  0.000000  0.000000   \n",
       "6      0.028159  0.021648  0.000000  ...      0.000000  0.000000  0.000000   \n",
       "7      0.000000  0.000000  0.000000  ...      0.000000  0.000000  0.000000   \n",
       "8      0.000000  0.031902  0.000000  ...      0.000000  0.000000  0.000000   \n",
       "9      0.000000  0.000000  0.000000  ...      0.189416  0.189416  0.189416   \n",
       "10     0.071678  0.000000  0.000000  ...      0.000000  0.000000  0.000000   \n",
       "\n",
       "        2020  consistently     ranks       one      most   popular  languages  \n",
       "0   0.000000      0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  \n",
       "1   0.000000      0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  \n",
       "2   0.000000      0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  \n",
       "3   0.000000      0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  \n",
       "4   0.000000      0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  \n",
       "5   0.000000      0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  \n",
       "6   0.000000      0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  \n",
       "7   0.000000      0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  \n",
       "8   0.000000      0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  \n",
       "9   0.189416      0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  \n",
       "10  0.000000      0.154977  0.154977  0.154977  0.154977  0.154977   0.154977  \n",
       "\n",
       "[11 rows x 108 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3 = Tokenizer()\n",
    "tfidf_test3 = TfidfVectorizer(test3)\n",
    "pd.DataFrame(tfidf_test3.fit_transform(wiki_python), columns=list(test3.word_dict.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "rs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
