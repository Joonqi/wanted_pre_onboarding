{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7da1364",
   "metadata": {},
   "source": [
    "# Pre On Boarding assingment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6303bd0",
   "metadata": {},
   "source": [
    "### 1) Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edb75543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        self.word_dict = {'oov' : 0}\n",
    "        self.fit_checker = False\n",
    "    \n",
    "    def preprocessing(self, sequences):\n",
    "        result = []\n",
    "        for sentence in sequences:\n",
    "            string = re.sub(r'[^a-zA-Z0-9 ]', '', sentence.lower())\n",
    "            result.append(string.split())\n",
    "        return result\n",
    "    \n",
    "    def fit(self, sequences):\n",
    "        self.fit_checker = False\n",
    "        tokens = self.preprocessing(sequences)\n",
    "        idx = 1\n",
    "        for token in tokens:\n",
    "            for word in token:\n",
    "                if word not in self.word_dict:\n",
    "                    self.word_dict[word] = idx\n",
    "                    idx += 1\n",
    "        self.fit_checker = True\n",
    "    \n",
    "    def transform(self, sequences):\n",
    "        result = []\n",
    "        tokens = self.preprocessing(sequences)\n",
    "        if self.fit_checker:\n",
    "            for token in tokens:\n",
    "                indexes = []\n",
    "                for word in token:\n",
    "                    if word in self.word_dict:\n",
    "                        indexes.append(self.word_dict[word])\n",
    "                    else:\n",
    "                        indexes.append(self.word_dict['oov'])\n",
    "                result.append(indexes)\n",
    "            return result\n",
    "        else:\n",
    "            raise Exception('Tokenizer instance is not fitted yet')\n",
    "    \n",
    "    def fit_transform(self, sequences):\n",
    "        self.fit(sequences)\n",
    "        result = self.transform(sequences)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "458234f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'go', 'to', 'school'], ['i', 'like', 'pizza'], ['i', 'dont', 'like', 'it']]\n"
     ]
    }
   ],
   "source": [
    "test = Tokenizer()\n",
    "test_input = ['I go to school.', 'I LIKE pizza!', \"I don't like it.\"]\n",
    "\n",
    "print(test.preprocessing(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1300e9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oov': 0, 'i': 1, 'go': 2, 'to': 3, 'school': 4, 'like': 5, 'pizza': 6}\n"
     ]
    }
   ],
   "source": [
    "test = Tokenizer()\n",
    "test_input = ['I go to school.', 'I LIKE pizza!']\n",
    "\n",
    "test.fit(test_input)\n",
    "print(test.word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f90f9187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4], [1, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "test = Tokenizer()\n",
    "test_input = ['I go to school.', 'I LIKE pizza!']\n",
    "\n",
    "test.fit(test_input)\n",
    "print(test.transform(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac7f28fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4], [1, 5, 6]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Tokenizer()\n",
    "test_input = ['I go to school.', 'I LIKE pizza!']\n",
    "\n",
    "test.fit_transform(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0746e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4803292f",
   "metadata": {},
   "source": [
    "### 2) TfIdf Vecotrizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "972338e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class TfidfVectorizer:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.fit_checker = False\n",
    "    \n",
    "    def fit(self, sequences):\n",
    "        tokenized = self.tokenizer.fit_transform(sequences)\n",
    "        N = len(tokenized)\n",
    "        words = list(self.tokenizer.word_dict.values())\n",
    "\n",
    "        self.idf_vector = [0]*len(words)\n",
    "        for i in range(len(words)):\n",
    "            df = 0\n",
    "            for sentence in tokenized:\n",
    "                if words[i] in sentence:\n",
    "                    df += 1\n",
    "            self.idf_vector[i] = math.log(N/(1+df))\n",
    "        self.fit_checker = True\n",
    "    \n",
    "    def transform(self, sequences):\n",
    "        if self.fit_checker:\n",
    "            tokenized = self.tokenizer.transform(sequences)\n",
    "            N = len(tokenized)\n",
    "            words = list(self.tokenizer.word_dict.values())\n",
    "            \n",
    "            self.tfidf_matrix = [[0]*len(words)for i in range(N)]\n",
    "            for i in range(N):\n",
    "                for j in range(len(words)):\n",
    "                    tfidf = tokenized[i].count(words[j]) * self.idf_vector[i]\n",
    "                    self.tfidf_matrix[i][j] = tfidf\n",
    "            \n",
    "            return self.tfidf_matrix\n",
    "        else:\n",
    "            raise Exception('TfidfVectorizer instance is not fitted yet')\n",
    "        \n",
    "    def fit_transform(self, sequences):\n",
    "        self.fit(sequences)\n",
    "        return self.transform(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83cf1433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6931471805599453, -0.40546510810816444, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = ['I go to school.', 'I LIKE pizza!']\n",
    "tfidf_test = TfidfVectorizer(test)\n",
    "\n",
    "tfidf_test.fit(test_input)\n",
    "tfidf_test.idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6d1ac043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, -0.40546510810816444, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, -0.40546510810816444, 0.0, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_test.transform(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b5877d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0,\n",
      "  0.22314355131420976,\n",
      "  0.5108256237659907,\n",
      "  0.22314355131420976,\n",
      "  0.9162907318741551,\n",
      "  0.9162907318741551,\n",
      "  0.9162907318741551,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0],\n",
      " [0.0,\n",
      "  0.22314355131420976,\n",
      "  0.0,\n",
      "  0.22314355131420976,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.9162907318741551,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0],\n",
      " [0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.22314355131420976,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.9162907318741551,\n",
      "  0.9162907318741551,\n",
      "  0.9162907318741551,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0],\n",
      " [0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.9162907318741551,\n",
      "  0.9162907318741551,\n",
      "  0.9162907318741551,\n",
      "  0.9162907318741551,\n",
      "  0.0,\n",
      "  0.0],\n",
      " [0.0,\n",
      "  0.22314355131420976,\n",
      "  0.5108256237659907,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.0,\n",
      "  0.9162907318741551,\n",
      "  0.9162907318741551]]\n",
      "{'deep': 11,\n",
      " 'do': 10,\n",
      " 'dont': 15,\n",
      " 'from': 13,\n",
      " 'i': 3,\n",
      " 'know': 2,\n",
      " 'learning': 12,\n",
      " 'like': 7,\n",
      " 'love': 6,\n",
      " 'me': 16,\n",
      " 'oov': 0,\n",
      " 'scratch': 14,\n",
      " 'should': 9,\n",
      " 'want': 4,\n",
      " 'what': 8,\n",
      " 'you': 1,\n",
      " 'your': 5}\n"
     ]
    }
   ],
   "source": [
    "test_input = ['You know I want your love',\n",
    "             'I like you',\n",
    "             'What should I do',\n",
    "             'Deep Learning from Scratch',\n",
    "             \"You don't know me\"]\n",
    "\n",
    "test2 = Tokenizer()\n",
    "tfidf_test2 = TfidfVectorizer(test2)\n",
    "from pprint import pprint\n",
    "pprint(tfidf_test2.fit_transform(test_input))\n",
    "pprint(tfidf_test2.tokenizer.word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582a76b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "rs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
